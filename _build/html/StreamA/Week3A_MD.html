

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>3. Matrices &#8212; PHY129 Mathematics for Physicists</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"HTML-CSS": {"matchFontHeight": true, "scale": 80}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'StreamA/Week3A_MD';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="2. Vectors" href="Week2A_MD.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../Overview.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="PHY129 Mathematics for Physicists - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="PHY129 Mathematics for Physicists - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../Overview.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Stream A - Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Week1A_MD.html">1. Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week2A_MD.html">2. Vectors</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. Matrices</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FStreamA/Week3A_MD.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/StreamA/Week3A_MD.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Matrices</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">3.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#manipulating-matrices">3.2. Manipulating matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transpose">3.2.1. Transpose</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complex-conjugate">3.2.2. Complex conjugate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hermitian-conjugate">3.2.3. Hermitian conjugate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-and-multiplying-matrices">3.3. Adding and multiplying matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#addition">3.3.1. Addition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar-multiplication">3.3.2. Scalar multiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication">3.3.3. Matrix multiplication</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#special-matrices-sec-special">3.4. Special matrices {#sec:special}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#square-matrices">3.4.1. Square matrices</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#identity-matrix">3.4.2. Identity matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-matrix">3.4.3. Real matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#symmetric-matrix">3.4.4. Symmetric matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hermitian-matrix">3.4.5. Hermitian matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse-matrix">3.4.6. Inverse matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unitary-matrix">3.4.7. Unitary matrix</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-determinant-sec-det">3.5. The determinant {#sec:det}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-cofactors-and-minor-determinants-sec-cofactors">3.5.1. Matrix cofactors and minor determinants {#sec:cofactors}</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-determinants">3.5.2. Properties of determinants</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse-of-a-matrix-sec-inverse">3.6. Inverse of a matrix {#sec:inverse}</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvalue-problems-sec-eigen">3.7. Eigenvalue problems {#sec:eigen}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvalues">3.7.1. Eigenvalues</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvectors">3.7.2. Eigenvectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-coupled-equations">3.8. Linear coupled equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trivial-solutions-trivial-solutions-unnumbered">3.8.1. Trivial solutions {#trivial-solutions .unnumbered}</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#homogeneous-equations">3.8.2. Homogeneous equations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inhomogeneous-equations">3.8.3. Inhomogeneous equations</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="matrices">
<h1><span class="section-number">3. </span>Matrices<a class="headerlink" href="#matrices" title="Permalink to this heading">#</a></h1>
<p>The material in this topic is also covered in chapter 9 of the textbook
Martin and Shaw “Mathematics for Physicists”. In Boas “Mathematical
Methods in the Physical Sciences” it is in chapter 3 sections 2,3,6 and
9.</p>
<section id="introduction">
<h2><span class="section-number">3.1. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>Matrices are used in many areas of physics including classical and
quantum mechanics. They are an indispensable tool for solving problems
in physics.</p>
<p>You already know that a scalar is a single number and a vector is a list
of numbers (a column or a row). A matric is an array of numbers on a
rectangular grid. For example this matrix has three rows and four
columns:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\label{eq:matrixA}
 A = 
 \begin{pmatrix} 
  3 &amp; 5 &amp; 1 &amp; 2 \cr 
  0 &amp; 1 &amp; 6 &amp; 5 \cr 
  4 &amp; 3 &amp; 7 &amp; 2 
 \end{pmatrix}\, .
\end{aligned}\]</div>
<p>Each individual number in the matrix is called a <em>matrix
element</em>. Row vectors can be thought of as matrices with only one row
and column vecotrs can be viewed as matrices with only one column. In
general a matrix is an of numbers arranged in <span class="math notranslate nohighlight">\(n\)</span> rows and <span class="math notranslate nohighlight">\(m\)</span> columns,
where <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(m\)</span> are whole positive numbers. The numbers that make up
the matrix elements can be real or complex. The numbers in a matrix can
be variables (like distance <span class="math notranslate nohighlight">\(x\)</span> or time <span class="math notranslate nohighlight">\(t\)</span> in physics equations) or
functions of variables.</p>
<p><em>Index notation</em> provides a convenient way of identifying a particular
element in a matrix. We achieve this by using subscripts—or
<em>indices</em>—that indicate the position of the element in the matrix. The
element</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\nonumber
 a_{jk}
\end{aligned}\]</div>
<p>is the number in the matrix <span class="math notranslate nohighlight">\(A\)</span> on the <span class="math notranslate nohighlight">\(j^{\text{th}}\)</span>
row and the <span class="math notranslate nohighlight">\(k^{\text{th}}\)</span> column. In these notes, I will use upper
case roman letters for matrices and their corresponding lower case for
its elements. The element <span class="math notranslate nohighlight">\(a_{jk}\)</span> is different from the matrix element
<span class="math notranslate nohighlight">\(a_{kj}\)</span>. In the matrix <span class="math notranslate nohighlight">\(A\)</span> above
(equation <span class="xref myst">[eq:matrixA]</span>{reference-type=”ref”
reference=”eq:matrixA”}), element <span class="math notranslate nohighlight">\(a_{12}=5\)</span> and <span class="math notranslate nohighlight">\(a_{21}=0\)</span>, so
<span class="math notranslate nohighlight">\(a_{12} \neq a_{21}\)</span>. The convention for the order of the indices is
<em>row-column</em>. It is crucial you get this the right way around in matrix
calculations. In general, we can write an <span class="math notranslate nohighlight">\(n\times m\)</span> (pronounced “<span class="math notranslate nohighlight">\(n\)</span>
<em>by</em> <span class="math notranslate nohighlight">\(m\)</span>”) matrix as</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A = 
 \begin{pmatrix} 
  a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1m} \cr 
  a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2m} \cr 
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \cr 
  a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nm}  
 \end{pmatrix}\, .
\end{aligned}\]</div>
<p>When <span class="math notranslate nohighlight">\(n=m\)</span>, we have a <em>square</em> matrix.</p>
</section>
<section id="manipulating-matrices">
<h2><span class="section-number">3.2. </span>Manipulating matrices<a class="headerlink" href="#manipulating-matrices" title="Permalink to this heading">#</a></h2>
<section id="transpose">
<h3><span class="section-number">3.2.1. </span>Transpose<a class="headerlink" href="#transpose" title="Permalink to this heading">#</a></h3>
<p>The <em>transpose</em> of a matrix is obtained by swapping the rows and the
columns in the matrix. So the matrix elements become:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 a_{jk} \xrightarrow{\rm transpose} a_{kj}
\end{aligned}\]</div>
<p>We denote the transpose of a matrix by a superscript
<span class="math notranslate nohighlight">\(T\)</span>. The transpose of the matrix <span class="math notranslate nohighlight">\(A\)</span>
(equation <span class="xref myst">[eq:matrixA]</span>{reference-type=”ref”
reference=”eq:matrixA”}) can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\label{eq:matrixAT}
 A^T = 
 \begin{pmatrix} 
  3 &amp; 0 &amp; 4  \cr 
  5 &amp; 1 &amp; 3  \cr 
  1 &amp; 6 &amp; 7  \cr
  2 &amp; 5 &amp; 2  
 \end{pmatrix}\, .
\end{aligned}\]</div>
<p>Note how the rows and coloumns have swapped in <span class="math notranslate nohighlight">\(A^T\)</span>
compared to <span class="math notranslate nohighlight">\(A\)</span>. We can find the transpose by reflecting the elements in
an imaginary mirrow along the diagonal from the top left element
(<span class="math notranslate nohighlight">\(a_{11}\)</span>) to the bottom right element (<span class="math notranslate nohighlight">\(a_{nn}\)</span> or <span class="math notranslate nohighlight">\(a_{mm}\)</span>, depending
on which is smaller, <span class="math notranslate nohighlight">\(n\)</span> or <span class="math notranslate nohighlight">\(m\)</span>).</p>
<p>Note that the transpose of <span class="math notranslate nohighlight">\(A^T\)</span> brings us back to <span class="math notranslate nohighlight">\(A\)</span> again. The
transpose of a column vector gives the equivalent row vector and the
transpose of a row vector gives the equivalent coloumn vector.</p>
</section>
<section id="complex-conjugate">
<h3><span class="section-number">3.2.2. </span>Complex conjugate<a class="headerlink" href="#complex-conjugate" title="Permalink to this heading">#</a></h3>
<p>The elements of a matrix can be complex, which means that we can take
the complex conjugate of the matrix as a whole. This is achieved by
taking the complex conjugate of every element individually i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 a_{jk} \xrightarrow{\rm c.c.} a_{jk}^*\, .
\end{aligned}\]</div>
<p>Since our example matrix <span class="math notranslate nohighlight">\(A\)</span> has only real numbers, we
see that <span class="math notranslate nohighlight">\(A^* = A\)</span>. A less trivial example would be</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\label{eq:matrixB}
 B = 
 \begin{pmatrix} 
  1 &amp; 2i &amp; 0  \cr 
  6-i &amp; 3 &amp; e^{2\pi i/7} 
 \end{pmatrix}\, ,
 \qquad\text{and}\qquad
 B^* = 
 \begin{pmatrix} 
  1 &amp; -2i &amp; 0  \cr 
  6+i &amp; 3 &amp; e^{-2\pi i/7} 
 \end{pmatrix}\, .
\end{aligned}\]</div>
<p>Each element stays in place, but is complex conjugated:
<span class="math notranslate nohighlight">\(b_{jk} \rightarrow b_{jk}^*\)</span>. Remember that to get a complex conjugate
you reverse the sign of the imaginary parts.</p>
</section>
<section id="hermitian-conjugate">
<h3><span class="section-number">3.2.3. </span>Hermitian conjugate<a class="headerlink" href="#hermitian-conjugate" title="Permalink to this heading">#</a></h3>
<p>We can combine the transpose and the complex conjugate together to form
the <em>H</em>ermitian conjugate (sometimes called the Hermitian <em>adjoint</em>). We
denote this by a dagger:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 B^\dagger = \left(B^*\right)^T = \left(B^T\right)^*\, .
\end{aligned}\]</div>
<p>It does not matter whether you take the transpose first,
or if you take the complex conjugate first. In terms of the indices, it
can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 b_{jk} \xrightarrow{\text{Hermitian conjugate}} b_{kj}^*\, .
\end{aligned}\]</div>
<p>The Hermitian conjugate of <span class="math notranslate nohighlight">\(B\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\label{eq:matrixBH}
 B^\dagger = 
 \begin{pmatrix} 
  1 &amp; 6+i  \cr 
  -2i &amp; 3 \cr 
  0 &amp; e^{-2\pi i/7} 
 \end{pmatrix}\, .
\end{aligned}\]</div>
<p>conjugate without the transpose, but to see why we have
wait until section <span class="xref myst">1.7</span>{reference-type=”ref”
reference=”sec:eigen”}.</p>
</section>
</section>
<section id="adding-and-multiplying-matrices">
<h2><span class="section-number">3.3. </span>Adding and multiplying matrices<a class="headerlink" href="#adding-and-multiplying-matrices" title="Permalink to this heading">#</a></h2>
<section id="addition">
<h3><span class="section-number">3.3.1. </span>Addition<a class="headerlink" href="#addition" title="Permalink to this heading">#</a></h3>
<p>We can add two matrices, but <em>only when they have the same size and
shape <span class="math notranslate nohighlight">\(n\times m\)</span></em>. So, for example, we cannot add <span class="math notranslate nohighlight">\(A\)</span>
(equation <span class="xref myst">[eq:matrixA]</span>{reference-type=”ref”
reference=”eq:matrixA”}) and <span class="math notranslate nohighlight">\(B\)</span>
(equation <span class="xref myst">[eq:matrixB]</span>{reference-type=”ref”
reference=”eq:matrixB”}), or <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A^T\)</span>
(equation <span class="xref myst">[eq:matrixAT]</span>{reference-type=”ref”
reference=”eq:matrixAT”}), or <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(B^\dagger\)</span>
(equation <span class="xref myst">[eq:matrixBH]</span>{reference-type=”ref”
reference=”eq:matrixBH”}). But we can add <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(B^*\)</span>
(equation <span class="xref myst">[eq:matrixB]</span>{reference-type=”ref”
reference=”eq:matrixB”}):</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 B + B^* = 
 \begin{pmatrix} 
  1 &amp; 2i &amp; 0  \cr 
  6-i &amp; 3 &amp; e^{2\pi i/7} 
 \end{pmatrix}
 +
 \begin{pmatrix} 
  1 &amp; -2i &amp; 0  \cr 
  6+i &amp; 3 &amp; e^{-2\pi i/7} 
 \end{pmatrix}
 =  
 \begin{pmatrix} 
  2 &amp; 0 &amp; 0  \cr 
  12 &amp; 6 &amp; 2\cos\left(\frac{2\pi}{7}\right) 
 \end{pmatrix}\, .
\end{aligned}\]</div>
<p>So adding two matrices is accomplished simply by adding
the matrix elements at each position. Another example is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 C = 
 \begin{pmatrix} 
  1 &amp; 2 \cr 
  0 &amp; 7  
 \end{pmatrix}\, ,
 \qquad
  D =
 \begin{pmatrix} 
  6 &amp; 1 \cr 
  -3 &amp; 2 
 \end{pmatrix}\, ,
 \qquad
 C + D=  
 \begin{pmatrix} 
  7 &amp; 3 \cr 
  -3 &amp; 9 
 \end{pmatrix}\, .
\end{aligned}\]</div>
<p>Addition of matrices is <em>commutative</em>, which means that
it does not matter whether you calculate <span class="math notranslate nohighlight">\(C+D\)</span> or <span class="math notranslate nohighlight">\(D+C\)</span>. This is because
the individual elements <span class="math notranslate nohighlight">\(c_{jk}\)</span> and <span class="math notranslate nohighlight">\(d_{jk}\)</span>—which are numbers—can
be added in any order. It is also <em>associative:</em> i.e.
<span class="math notranslate nohighlight">\((A+B)+C = A+(B+C)\)</span>.</p>
</section>
<section id="scalar-multiplication">
<h3><span class="section-number">3.3.2. </span>Scalar multiplication<a class="headerlink" href="#scalar-multiplication" title="Permalink to this heading">#</a></h3>
<p>If we have a matrix <span class="math notranslate nohighlight">\(A\)</span>, we can also add it to itself to obtain
<span class="math notranslate nohighlight">\(A + A = 2A\)</span>. This implies that we can multiply a matrix by a number, or
<em>scalar</em>, <span class="math notranslate nohighlight">\(\lambda\)</span> that multiplies each element of <span class="math notranslate nohighlight">\(A\)</span> i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 \lambda A = 
 \begin{pmatrix} 
  \lambda a_{11} &amp; \lambda a_{12} &amp; \cdots &amp; \lambda a_{1m} \cr 
  \lambda a_{21} &amp; \lambda a_{22} &amp; \cdots &amp; \lambda a_{2m} \cr 
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \cr 
  \lambda a_{n1} &amp; \lambda a_{n2} &amp; \cdots &amp; \lambda a_{nm}  
 \end{pmatrix}\, .
\end{aligned}\]</div>
<p>This is called scalar multiplication. Scalar
multiplication is <em>distributive:</em> i.e.
<span class="math notranslate nohighlight">\(\lambda(A+B) = \lambda A + \lambda B\)</span>.</p>
<p>elements. For example, we can say that for any two <span class="math notranslate nohighlight">\(n\times m\)</span> matrices
<span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> with <span class="math notranslate nohighlight">\(D=A+B\)</span>, each element of <span class="math notranslate nohighlight">\(D\)</span> can be written as and <span class="math notranslate nohighlight">\(k\)</span>
free variables) fully describes the behaviour of the entire matrix,
because the matrix is merely the array of its elements (i.e., values and
their positions). Expressing the commutative and distributive law in
terms of the elements in equations
(<span class="xref myst">[eq:nbt5340wi]</span>{reference-type=”ref”
reference=”eq:nbt5340wi”}) and
(<span class="xref myst">[eq:b943whfi]</span>{reference-type=”ref”
reference=”eq:b943whfi”}) is called <em>index notation</em>. We will be using
it here, because it is a very convenient and compact notation, and
consequently it is used extensively in physics. Even though you may not
encounter index notation much this year, we will come back to this in
modules on relativity, electromagnetism, and quantum mechanics in later
years.</p>
</section>
<section id="matrix-multiplication">
<h3><span class="section-number">3.3.3. </span>Matrix multiplication<a class="headerlink" href="#matrix-multiplication" title="Permalink to this heading">#</a></h3>
<p>Multiplying two vectors together is difference from addition. Let us
start with an exmple of multiplying a <span class="math notranslate nohighlight">\(1\times 2\)</span> matrix and a
<span class="math notranslate nohighlight">\(2\times 1\)</span> matrix. These are actually just a row vector and a column
vector. To multiply them we take the scalar (dot) product of the row
from the first matrix and column from the second e.g. for
<span class="math notranslate nohighlight">\(\vec{a}=(2\; 4)\)</span> and <span class="math notranslate nohighlight">\(\vec{b} = \begin{pmatrix} 1 \\ 3 
\end{pmatrix}\)</span> we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\label{eq:bg94oweisnkj}
 \vec{a}\cdot\vec{b} = \begin{pmatrix}
  2 &amp; 4 
 \end{pmatrix}
 \begin{pmatrix}
  1  \\  3 
\end{pmatrix}
=2 \times 1+4 \times3 = 14\, .
\end{aligned}\end{split}\]</div>
<p>This is only possible if the length of the row of
<span class="math notranslate nohighlight">\(\vec{a}\)</span> is equal to the length of the column of <span class="math notranslate nohighlight">\(\vec{b}\)</span>. The
outcome of the dot or scalar product is a <span class="math notranslate nohighlight">\(1\times1\)</span> “matrix” with one
row and one column, i.e. just a number. This scalar product is also
called the <em>inner product</em> and is sometimes denoted
<span class="math notranslate nohighlight">\(\langle\vec{a},\vec{b}\rangle\)</span> or <span class="math notranslate nohighlight">\((\vec{a},\vec{b})\)</span>. The notation
<span class="math notranslate nohighlight">\(\langle\vec{a}|\vec{b}\rangle\)</span> is also used in quantum mechanics. We
can write the scalar or inner product as</p>
<div class="math notranslate nohighlight">
\[\langle\vec{a},\vec{b}\rangle=\sum_{j=1}^{n}a_j^*b_j\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is
the number of dimensions, <span class="math notranslate nohighlight">\(\vec{a}\)</span> is a row vector and <span class="math notranslate nohighlight">\(\vec{b}\)</span> is a
column vector. Note that if we are in complex space (i.e some elements
of the vectors are complex) then we need to take the complex conjugate
of <span class="math notranslate nohighlight">\(\vec{a}\)</span>. The vector on the left (<span class="math notranslate nohighlight">\(\vec{a}\)</span> in this case) is
sometimes called the <em>dual</em> i.e. scalar products are between a vector
and a dual vector whereby the dual vector of a column vector is found by
taking the Hermitian conjugate (i.e. the transpose to convert it to a
row vector and the complex conjugate).</p>
<p>The matrix multiplication <span class="math notranslate nohighlight">\(AB\)</span> of two matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> works in a
very similar way to the scalar product, but instead of a row vector and
a column vector, we now take a row (<span class="math notranslate nohighlight">\(j\)</span>) from matrix <span class="math notranslate nohighlight">\(A\)</span> and a column
(<span class="math notranslate nohighlight">\(k\)</span>) from matrix <span class="math notranslate nohighlight">\(B\)</span>. The resulting number (the scalar product) is the
element at the row position <span class="math notranslate nohighlight">\(j\)</span> and the column position <span class="math notranslate nohighlight">\(k\)</span>. We repeat
this for each combination of rows and columns of matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>,
respectively. As an example, let us try to multiply the two matrices</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 A = 
 \begin{pmatrix}
 8 &amp; 3 &amp; 2 \\ 2 &amp; 5 &amp; 7
\end{pmatrix}  
 \qquad\text{and}\qquad 
 B=
 \begin{pmatrix}
 12 &amp; 0 \\ 1 &amp; 13
\end{pmatrix}\, . 
\end{aligned}\end{split}\]</div>
<p>First, there are two possibilities for how to order the
product:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 AB =
 \begin{pmatrix}
 8 &amp; 3 &amp; 2 \\ 2 &amp; 5 &amp; 7
\end{pmatrix} 
\begin{pmatrix}
 12 &amp; 0 \\ 1 &amp; 13
\end{pmatrix} 
\quad\text{or}\quad
BA =
\begin{pmatrix}
 12 &amp; 0 \\ 1 &amp; 13
\end{pmatrix} 
 \begin{pmatrix}
 8 &amp; 3 &amp; 2 \\ 2 &amp; 5 &amp; 7
\end{pmatrix} .
\end{aligned}\end{split}\]</div>
<p>The length of the rows of the first matrix must equal
the length of the columns of the second for the multiplication to be
possible, and therefore only the matrix multiplication <span class="math notranslate nohighlight">\(BA\)</span> makes sense.
To calculate the product of these matrices you might find the following
way of visualising it helpful:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \phantom{
 \begin{pmatrix}
 12 &amp;0 \\ 1 &amp; 13
\end{pmatrix} 
}
 \begin{pmatrix}
 {8} &amp; 3 &amp; 2 \\ {2} &amp; 5 &amp; 7
\end{pmatrix} &amp;  \cr
 \begin{pmatrix}
 12 &amp;0 \\ 1 &amp; 13
\end{pmatrix} 
 \color{white}{
 \begin{pmatrix}
 \phantom{\cdot} &amp; \color{black}{\vdots} &amp; \!\phantom{2} \\ \!\color{black}{\cdot\cdot}\!\!\!\! &amp; \color{black}{\mathbf{68}} &amp; \!\phantom{7}
\end{pmatrix}
}
&amp; =
\begin{pmatrix}
 {96} &amp; 36 &amp; 24 \\ 34 &amp; \mathbf{68} &amp; 93
\end{pmatrix} .
\end{aligned}\end{split}\]</div>
<p>We can now multiply the two vectors <span class="math notranslate nohighlight">\(\vec{a}\)</span> and <span class="math notranslate nohighlight">\(\vec{b}\)</span> in the
other order i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \vec{b}\cdot\vec{a} = 
\begin{pmatrix}
  1  \\  3 
\end{pmatrix}
\begin{pmatrix}
  2 &amp; 4 
\end{pmatrix}
=\begin{pmatrix}
2 &amp; 4 \\
6 &amp; 12
\end{pmatrix}
\end{aligned}\end{split}\]</div>
<p>This is the <em>outer product</em> of two vectors with the
result being a matrix.</p>
<p>We can also write matrix multiplication using index notation. If <span class="math notranslate nohighlight">\(AB=C\)</span>,
the element <span class="math notranslate nohighlight">\(c_{jk}\)</span> is determined by the dot product of the
<span class="math notranslate nohighlight">\(j^{\rm th}\)</span> row of <span class="math notranslate nohighlight">\(A\)</span> and the <span class="math notranslate nohighlight">\(k^{\rm th}\)</span> column of <span class="math notranslate nohighlight">\(B\)</span>. We can write
this as</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 c_{jk} = \sum_{l=1}^m a_{jl} b_{lk}\, ,
\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(m\)</span> is the number of columns in <span class="math notranslate nohighlight">\(A\)</span> and the number
of rows in <span class="math notranslate nohighlight">\(B\)</span> (they must be the same for matrix multiplication to be
possible).</p>
<p>We have already seen that the order in which we multiply matrices
matters. An interesting thing happens when we take the Hermitian
conjugate of the product of two matrices. We find that:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 (AB)^\dagger = B^\dagger A^\dagger\, .
\end{aligned}\]</div>
<p>The order of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> changes! We can see this
explicitly by writing this equation in index notation:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 (AB)^\dagger_{jk} = \left( \sum_{l=1}^n a_{jl} b_{lk} \right)^\dagger = \sum_{l=1}^n a_{jl}^* b_{lk}^* = \sum_{l=1}^n \tilde{a}_{lj} \tilde{b}_{kl} =  \sum_{l=1}^n  \tilde{b}_{kl} \tilde{a}_{lj} = 
(B^\dagger A^\dagger)_{jk}\, ,
\end{aligned}\]</div>
<p>where we have used <span class="math notranslate nohighlight">\(\tilde{a}_{jk}\)</span> and <span class="math notranslate nohighlight">\(\tilde{b}_{jk}\)</span>
to represent the matrix elements of <span class="math notranslate nohighlight">\(A^\dagger\)</span> and <span class="math notranslate nohighlight">\(B^\dagger\)</span>,
respectively. <strong>I will not examine you on this proof, I am including it
here for completeness</strong>. Pay close attention to the positions of the
indices. We can swap the order of elements because they are simply
numbers, but a matrix product must always have the same index label on
the column position of the first and the row position of the second.</p>
<p>You will need to practice multiplying matrices lots, until you are
comfortable with it. We use matrix multiplication a lot in different
areas of physics e.g. classical, quantum, particle etc.</p>
<p>physics modules in later years. You should also be able to recognise
matrix multiplication when you encounter it in index notation. Note the
sum over <span class="math notranslate nohighlight">\(l\)</span> and the position of the indices. Matrix multiplication is
explained graphically <a class="reference external" href="https://www.youtube.com/watch?v=XkY2DOUCWMU&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&amp;index=5">in this video by
3Blue1Brown</a>.</p>
</section>
</section>
<section id="special-matrices-sec-special">
<h2><span class="section-number">3.4. </span>Special matrices {#sec:special}<a class="headerlink" href="#special-matrices-sec-special" title="Permalink to this heading">#</a></h2>
<p>In the previous sections we covered the basic techniques of matrix
algebra. Here, we will consider some special types of matrices.</p>
<section id="square-matrices">
<h3><span class="section-number">3.4.1. </span>Square matrices<a class="headerlink" href="#square-matrices" title="Permalink to this heading">#</a></h3>
<p>When the size of a matrix is <span class="math notranslate nohighlight">\(n\times n\)</span>, we call the matrix <em>square</em>.
Since two square matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are of the same size, we can
multiply them in two ways <span class="math notranslate nohighlight">\(AB\)</span> or <span class="math notranslate nohighlight">\(BA\)</span> and obtain a matrix of the same
size as <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. However, in general <span class="math notranslate nohighlight">\(AB\)</span> is not the same as <span class="math notranslate nohighlight">\(BA\)</span>.</p>
<p>In the special case where <span class="math notranslate nohighlight">\(AB = BA\)</span>, the matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are said
to <em>commute</em>. We can also construct a new matrix, called the
<em>commutator</em> of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
= AB - BA \, .
\end{aligned}\]</div>
<p>Matrices commute if the commutator is zero. In quantum
mechanics, matrices that do not commute are related to uncertainty
relations.</p>
<p>The <em>trace</em> of an <span class="math notranslate nohighlight">\(n\times n\)</span> square matrix is simply the sum of the
diagonal (top left to bottom right) elements i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 \tr A = \sum_{j=1}^n a_{jj}\, .
\end{aligned}\]</div>
</section>
<section id="identity-matrix">
<h3><span class="section-number">3.4.2. </span>Identity matrix<a class="headerlink" href="#identity-matrix" title="Permalink to this heading">#</a></h3>
<p>The identity matrix <span class="math notranslate nohighlight">\(I\)</span> is defined as the square matrix that when
multiplied with any other matrix of the same size returns that matrix
i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 IA = A \qquad \text{and}\qquad AI = A\, .
\end{aligned}\]</div>
<p>In matrix form, the identity matrix has zeros everywhere
and ones on the diagonal (from the top left to the bottom right). The
identity matrix is therefore the matrix multiplication equivalent to the
number 1.</p>
<p>For example, here are the <span class="math notranslate nohighlight">\(2\times 2\)</span> and <span class="math notranslate nohighlight">\(3\times 3\)</span> identity matrices:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 I_2= 
 \begin{pmatrix}
 1 &amp; 0 \\ 0 &amp; 1
\end{pmatrix}  
\qquad\text{and}\qquad
 I_3= 
 \begin{pmatrix}
 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 
\end{pmatrix} \, . 
\end{aligned}\end{split}\]</div>
<p>The elements of the identity matrix can be denoted by
<span class="math notranslate nohighlight">\(\delta_{jk}\)</span>, which is called the <em>Kronecker delta</em>, given by</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 \delta_{jk} = 
 \begin{cases}
  0 &amp; \text{if}~ j \neq k\, , \cr
  1 &amp; \text{if}~ j = k\, .
 \end{cases}
\end{aligned}\]</div>
<p>it may look intimidating now.</p>
</section>
<section id="real-matrix">
<h3><span class="section-number">3.4.3. </span>Real matrix<a class="headerlink" href="#real-matrix" title="Permalink to this heading">#</a></h3>
<p>When all the elements in a matrix are real, we call the matrix a real
matrix i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A^* = A \, .
\end{aligned}\]</div>
<p>Similarly, when the elements are complex we call the
matrix a complex matrix.</p>
</section>
<section id="symmetric-matrix">
<h3><span class="section-number">3.4.4. </span>Symmetric matrix<a class="headerlink" href="#symmetric-matrix" title="Permalink to this heading">#</a></h3>
<p>Symmetric matrices are matrices that are invariant (i.e., does not
change) after taking the transpose i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A^T = A\, .
\end{aligned}\]</div>
<p>In index notation we express this as <span class="math notranslate nohighlight">\(a_{jk} = a_{kj}\)</span>.
You can recognise symmetric matrices by spotting mirror symmetry in the
diagonal.</p>
</section>
<section id="hermitian-matrix">
<h3><span class="section-number">3.4.5. </span>Hermitian matrix<a class="headerlink" href="#hermitian-matrix" title="Permalink to this heading">#</a></h3>
<p>When the Hermitian conjugate of a matrix is identical to that matrix,
the matrix is called Hermitian:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A^\dagger = A\, .
\end{aligned}\]</div>
<p>In section <span class="xref myst">1.7</span>{reference-type=”ref”
reference=”sec:eigen”} we will learn about the eigenvalues of a matrix,
and Hermitian matrices always have real eigenvalues even though the
elements of Hermitian matrices are not necessarily real. This property
of having real eigenvalues makes them of special interest in physics. An
example of Hermitian matrices used in quantum mechanics are the Pauli
matrices:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 \sigma_x = 
 \begin{pmatrix}
  0 &amp; 1 \cr 1 &amp; 0
 \end{pmatrix}\, ,
 \qquad
 \sigma_y = 
 \begin{pmatrix}
  0 &amp; -i \cr i &amp; 0
 \end{pmatrix}\, ,
 \qquad\text{and}\qquad
 \sigma_z = 
 \begin{pmatrix}
  1 &amp; 0 \cr 0 &amp; -1
 \end{pmatrix}\, .
\end{aligned}\]</div>
</section>
<section id="inverse-matrix">
<h3><span class="section-number">3.4.6. </span>Inverse matrix<a class="headerlink" href="#inverse-matrix" title="Permalink to this heading">#</a></h3>
<p>The inverse of a square matrix <span class="math notranslate nohighlight">\(A\)</span> is denoted by <span class="math notranslate nohighlight">\(A^{-1}\)</span>, and is the
matrix that when multiplied by <span class="math notranslate nohighlight">\(A\)</span> gives the identity matrix, i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A^{-1} A = A A^{-1} = I \, .
\end{aligned}\]</div>
<p>It is the matrix generalisation of multiplying a number
<span class="math notranslate nohighlight">\(\lambda\)</span> with its reciprocal value <span class="math notranslate nohighlight">\(\lambda^{-1} = 1/\lambda\)</span>. We write
the inverse with the superscript <span class="math notranslate nohighlight">\(-1\)</span> to avoid ambiguity in the order of
the matrices. If we were to write <span class="math notranslate nohighlight">\(B/A\)</span>, it would be unclear whether we
meant <span class="math notranslate nohighlight">\(A^{-1} B\)</span> or <span class="math notranslate nohighlight">\(B A^{-1}\)</span>. Both are valid expressions, but in
general give different results. If we take the inverse of an inverse, we
get the original matrix again: <span class="math notranslate nohighlight">\((A^{-1})^{-1} = A\)</span>.</p>
<p>Sometimes the inverse of a matrix does not exist, just like the inverse
of <span class="math notranslate nohighlight">\(\lambda = 0\)</span> does not exist. We will study ways to calculate the
inverse in section <span class="xref myst">1.6</span>{reference-type=”ref”
reference=”sec:inverse”} and identify ways to tell whether the inverse
exists.</p>
</section>
<section id="unitary-matrix">
<h3><span class="section-number">3.4.7. </span>Unitary matrix<a class="headerlink" href="#unitary-matrix" title="Permalink to this heading">#</a></h3>
<p>A unitary matrix is defined by the property</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A^{-1} = A^\dagger \, .
\end{aligned}\]</div>
<p>These types of matrices play a central role in
coordinate transformations that preserve the angles between unit
vectors. Real-valued unitary matrices describe rotations and
reflections, while complex unitary matrices are used to describe the
evolution of quantum systems.</p>
</section>
</section>
<section id="the-determinant-sec-det">
<h2><span class="section-number">3.5. </span>The determinant {#sec:det}<a class="headerlink" href="#the-determinant-sec-det" title="Permalink to this heading">#</a></h2>
<p>The <em>determinant</em> is like the magnitude of the matrix. The determinant
is useful because it helps us to find the inverse of a matrix. A matrix
has to be square to be able to calculate it’s determinant e.g. (a 2x2,
3x3 matrix).</p>
<!-- the transformation, the area is no longer a square, because the
horizontal length has become $3$ and the vertical length has become $2$.
The new area is therefore $6$ (see
figure [\[fig:det\]](#fig:det){reference-type="ref"
reference="fig:det"}). three dimensions the determinant is the scaling
factor for volume, and so on. When a determinant is zero, the matrix
transformation changes the area to a line or point, and a volume to an
area, line or point. In other words, the volumes get mapped to a lower
dimension. Negative determinants occur when the transformation changes
the orientation of the basis vectors (e.g., swap $\hat{\rm e}_x$ and
$\hat{\rm e}_y$) which for $2D$ can be visualised as flipping the $2D$
plane to the other side like turning over a sheet of paper. -->
<p>To calculate the determinant of a (2x2) matrix, we use the following
formula</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \det 
  \begin{pmatrix}
  a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22}
 \end{pmatrix}
 \equiv
  \begin{vmatrix}
  a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22}
 \end{vmatrix}
 = a_{11}a_{22} - a_{12}a_{21}\, .
\end{aligned}\end{split}\]</div>
<p>You should memorise this formula, because you will use
it a lot.</p>
<p>An intuitive explanation of the concept of the determinant is given <a class="reference external" href="https://www.youtube.com/watch?v=Ip3X9LOh2dk">in
this video by 3Blue1Brown</a>.</p>
<section id="matrix-cofactors-and-minor-determinants-sec-cofactors">
<h3><span class="section-number">3.5.1. </span>Matrix cofactors and minor determinants {#sec:cofactors}<a class="headerlink" href="#matrix-cofactors-and-minor-determinants-sec-cofactors" title="Permalink to this heading">#</a></h3>
<p>Calculating the determinant of a three-dimensional matrix such as;</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\label{eq:dfb429wehuisfnj}
 A =
  \begin{pmatrix}
  a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \\a_{31} &amp; a_{32} &amp; a_{33}
 \end{pmatrix}\, ,
\end{aligned}\end{split}\]</div>
<p>is more complicated than for a <span class="math notranslate nohighlight">\(2\times 2\)</span> matrix. Let
us first introduce two concepts that will help us find the <span class="math notranslate nohighlight">\(3\times 3\)</span>
determinant: the minor determinant and the cofactor of a matrix element.</p>
<p>Pick an element <span class="math notranslate nohighlight">\(a_{jk}\)</span> of an <span class="math notranslate nohighlight">\(n\times n\)</span> matrix. It determines a row
<span class="math notranslate nohighlight">\(j\)</span> and a column <span class="math notranslate nohighlight">\(k\)</span>, namely its position in the matrix. If you remove
that row and column, you end up with an <span class="math notranslate nohighlight">\((n-1)\times 
(n-1)\)</span> matrix. The determinant of this smaller matrix is called the
<em>minor determinant</em> corresponding to the element, and we denote it by
<span class="math notranslate nohighlight">\(m_{jk}\)</span>. To obtain the cofactor of that element, we multiply the minor
determinant with a factor <span class="math notranslate nohighlight">\((-1)^{j+k}\)</span>. This produces a positive sign
when <span class="math notranslate nohighlight">\(j+k\)</span> is even, and a minus sign when <span class="math notranslate nohighlight">\(j+k\)</span> is odd. This looks like
a checker (or chess) board:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\nonumber
 (-1)^{j+k}:\quad
 \begin{pmatrix}
  +&amp;-&amp;+ \\ -&amp;+&amp;- \\ +&amp;-&amp;+
 \end{pmatrix}\, .
\end{aligned}\end{split}\]</div>
<p>We can therefore write the cofactor of <span class="math notranslate nohighlight">\(a_{jk}\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\label{eq:cofactor}
 C_{jk} = (-1)^{j+k} m_{jk}\, .
\end{aligned}\]</div>
<p>We will use the matrix cofactors, <span class="math notranslate nohighlight">\(C\)</span>, to calculate the
determinant of larger matrices.</p>
<p>To calculate the determinant of <span class="math notranslate nohighlight">\(A\)</span> in equation
(<span class="xref myst">[eq:dfb429wehuisfnj]</span>{reference-type=”ref”
reference=”eq:dfb429wehuisfnj”}), we can use the so-called <em>Laplace
rule</em>: We start by picking a row or a column (it does not matter which
one we pick). Then we multiply each element in that row or column by its
cofactor. The determinant is the sum of these products. For example,
let’s pick the top row of matrix <span class="math notranslate nohighlight">\(A\)</span> to calculate the determinant:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \det A 
 &amp; = a_{11} A_{11} + a_{12} A_{12} + a_{13} A_{13} \cr 
 &amp; = a_{11} 
  \begin{vmatrix}
  a_{22} &amp; a_{23} \\ a_{32} &amp; a_{33}
 \end{vmatrix}
 - a_{12} 
  \begin{vmatrix}
  a_{21} &amp; a_{23} \\ a_{31} &amp; a_{33}
 \end{vmatrix}
 + a_{13}
  \begin{vmatrix}
  a_{21} &amp; a_{22} \\ a_{31} &amp; a_{32}
 \end{vmatrix}\, .
\end{aligned}\end{split}\]</div>
<p>As a numerical example, consider</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \det A = 
  \begin{vmatrix}
  1 &amp; 2 &amp; 4 \\ 0 &amp; 1 &amp; 3 \\ 2 &amp; 5 &amp; 1
 \end{vmatrix}\, .
\end{aligned}\end{split}\]</div>
<p>Since there is a zero in the matrix, it is useful to
pick a row or column that includes it, since it will reduce the number
of determinants we need to calculate. Let’s pick the second row. The
determinant is then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \det A = 
 0 \times X - 1 \times 
  \begin{vmatrix}
  1 &amp; 4 \\ 2 &amp; 1
 \end{vmatrix}
 + 3 \times
  \begin{vmatrix}
  1 &amp; 2 \\ 2 &amp; 5
 \end{vmatrix} 
 = -1\times (-7) + 3 \times 1= 10\, .
\end{aligned}\end{split}\]</div>
<p>We do not bother to calculate the determinant <span class="math notranslate nohighlight">\(X\)</span>
because it gets multiplied by zero anyway. Calculating determinants is
one of those things, along with matrix multiplication, that you should
practice lots so you become comfortable with it.</p>
<p>We can use the determinant to calculate the cross product of two vectors
<span class="math notranslate nohighlight">\(\vec{a}\)</span> and <span class="math notranslate nohighlight">\(\vec{b}\)</span>. We construct the following matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\label{eq:b9woiersfkjn}
  \begin{pmatrix}
  \hat{\vec{e}}_x &amp; \hat{\vec{e}}_y &amp; \hat{\vec{e}}_z \\ a_x &amp; a_y &amp; a_z \\ b_x &amp; b_y &amp; b_z
 \end{pmatrix}\, .
\end{aligned}\end{split}\]</div>
<p>Here, instead of numbers in the top row, we entered
vectors so this is not a proper matrix. However if we pick the top row
and follow the method of calculating a determinant this will give us the
cross product</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \a\times\b =
  \begin{vmatrix}
  \hat{\vec{e}}_x &amp; \hat{\vec{e}}_y &amp; \hat{\vec{e}}_z \\ a_x &amp; a_y &amp; a_z \\ b_x &amp; b_y &amp; b_z
 \end{vmatrix}
 = (a_y b_z - a_z b_y)\, \hat{\vec{e}}_x - (a_x b_z - a_z b_x)\, \hat{\vec{e}}_y + (a_x b_y - a_y b_x)\, \hat{\vec{e}}_z\, .
\end{aligned}\end{split}\]</div>
<p>Why is this related to the determinant? The magnitude of
<span class="math notranslate nohighlight">\(\vec{a}\times\vec{b}\)</span> is the area of the parallelogram spanned by
<span class="math notranslate nohighlight">\(\vec{a}\)</span> and <span class="math notranslate nohighlight">\(\vec{b}\)</span>. If we take the dot product of
<span class="math notranslate nohighlight">\(\vec{a}\times\vec{b}\)</span> with a vector <span class="math notranslate nohighlight">\(\vec{c}\)</span> we get a number that
is the volume of the parallelepiped formed by <span class="math notranslate nohighlight">\(\vec{a}\)</span>, <span class="math notranslate nohighlight">\(\vec{b}\)</span>,
and <span class="math notranslate nohighlight">\(\vec{c}\)</span>. We could therefore replace the unit vectors in equation
(<span class="xref myst">[eq:b9woiersfkjn]</span>{reference-type=”ref”
reference=”eq:b9woiersfkjn”}) with <span class="math notranslate nohighlight">\(c_x\)</span>, <span class="math notranslate nohighlight">\(c_y\)</span>, and <span class="math notranslate nohighlight">\(c_z\)</span>, and then the
determinant is the volume change of the transformation that takes the
unit cube to the parallelepiped. also <a class="reference external" href="https://www.youtube.com/watch?v=eu6i7WJeinw&amp;index=11&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">this video by
3Blue1Brown</a>
and <a class="reference external" href="https://www.youtube.com/watch?v=BaM7OCEm3G0&amp;index=12&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">its
sequel</a>.</p>
</section>
<section id="properties-of-determinants">
<h3><span class="section-number">3.5.2. </span>Properties of determinants<a class="headerlink" href="#properties-of-determinants" title="Permalink to this heading">#</a></h3>
<p>Some important properties of the determinant are the following:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\det (A^T) = \det A\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\det AB = \det A \times \det B\)</span>. This implies that
<span class="math notranslate nohighlight">\(\det AB = \det BA\)</span>, even when <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> do not commute</p></li>
<li><p><span class="math notranslate nohighlight">\(\det (A^{-1}) = (\det A)^{-1}\)</span></p></li>
<li><p>if you interchange two rows or two columns, the determinant changes
sign</p></li>
<li><p>if two rows are identical, the determinant is zero</p></li>
<li><p>if you multiply a row or a column by a factor <span class="math notranslate nohighlight">\(\lambda\)</span>, then the
determinant is multiplied by <span class="math notranslate nohighlight">\(\lambda\)</span></p></li>
<li><p>from the previous two properties it follows that if two rows or
columns are <em>proportional</em> to each other, the determinant is zero</p></li>
<li><p>adding two rows or columns, and replacing one of them with the
resulting row or column with not change the determinant</p></li>
<li><p>the determinant of a diagonal matrix is the product of the elements
on the diagonal.</p></li>
</ul>
<p>The first three properties are very useful in algebraic manipulation,
while the remaining properties are very useful in simplifying the
calculation of determinants of large matrices.</p>
<p>interesting types of matrices: rotation: just calculate the determinant.
If it is <span class="math notranslate nohighlight">\(+1\)</span> the reflections created an effective rotation, if it is
<span class="math notranslate nohighlight">\(-1\)</span> we have a true reflection. some phase <span class="math notranslate nohighlight">\(\phi\)</span> in the interval
<span class="math notranslate nohighlight">\([0,2\pi)\)</span>.</p>
</section>
</section>
<section id="inverse-of-a-matrix-sec-inverse">
<h2><span class="section-number">3.6. </span>Inverse of a matrix {#sec:inverse}<a class="headerlink" href="#inverse-of-a-matrix-sec-inverse" title="Permalink to this heading">#</a></h2>
<p>In this section we learn how to calculate the inverse of a matrix.
Remember that the inverse of a matrix <span class="math notranslate nohighlight">\(A\)</span> (if it has one) is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A A^{-1} = A^{-1} A = I
\end{aligned}\]</div>
<p>and therefore also <span class="math notranslate nohighlight">\(\det A^{-1} = (\det A)^{-1}\)</span>. From
this we can see that only square matrices have an inverse and that a
matrix which has a zero determinant does not have an inverse. An
<em>invertible</em> matrix has an inverse. If a matrix does not have an inverse
we call it <em>singluar</em>.</p>
<p>One way to calculate the inverse of a matrix, <span class="math notranslate nohighlight">\(A^{-1}\)</span>, is to first
calculate the matrix of cofactors, <span class="math notranslate nohighlight">\(C\)</span>, (see
section <span class="xref myst">1.5.1</span>{reference-type=”ref”
reference=”sec:cofactors”}). We then caclulate the determinant of <span class="math notranslate nohighlight">\(A\)</span>
and the transpose of the matrix of cofactors, <span class="math notranslate nohighlight">\(C^T\)</span>. <span class="math notranslate nohighlight">\(C^{T}\)</span> is
sometimes called the adjoint of the matrix i.e. <span class="math notranslate nohighlight">\(C^T=\adj A\)</span>. This is
not the same as the Hermitian adjoint operator <span class="math notranslate nohighlight">\(\dagger\)</span> from section
<span class="xref myst">1.4</span>{reference-type=”ref” reference=”sec:special”}.
Finding the matrix of cofactors <span class="math notranslate nohighlight">\(C\)</span> can be a lot of work. The inverse is
then given by;</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
A^{-1} = \frac{C^T}{\det A}
\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(C_{ij}\)</span> is the cofactor of <span class="math notranslate nohighlight">\(A_{ij}\)</span> as given in
equation <span class="xref myst">[eq:cofactor]</span>{reference-type=”ref”
reference=”eq:cofactor”}. For a proof of why this method works see
Appendix <span class="xref myst">[sec:adjointmethod]</span>{reference-type=”ref”
reference=”sec:adjointmethod”}.</p>
</section>
<section id="eigenvalue-problems-sec-eigen">
<h2><span class="section-number">3.7. </span>Eigenvalue problems {#sec:eigen}<a class="headerlink" href="#eigenvalue-problems-sec-eigen" title="Permalink to this heading">#</a></h2>
<p>A matrix, <span class="math notranslate nohighlight">\(A\)</span>, changes the direction of the basis vectors
(<span class="math notranslate nohighlight">\(\hat{\vec{x}}\)</span> and <span class="math notranslate nohighlight">\(\hat{\vec{y}}\)</span> in 2D), but there are also two
(in 2D) vectors that do not change direction under the action of <span class="math notranslate nohighlight">\(A\)</span>;
they only scale (change length) i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A\vec{u} = \lambda_u\vec{u} \qquad\text{and}\qquad A\vec{v} =  \lambda_v \vec{v}
\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_u\)</span> and <span class="math notranslate nohighlight">\(\lambda_v\)</span> are scalars which we
call the <em>eigenvalues</em> of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(\vec{u}\)</span> and <span class="math notranslate nohighlight">\(\vec{v}\)</span> are vectors
we call the <em>eignevectors</em>. The eigenvectors are the directions along
which <span class="math notranslate nohighlight">\(A\)</span> produces a simple scaling. if we write the matrix <span class="math notranslate nohighlight">\(A\)</span> in the
basis that points along <span class="math notranslate nohighlight">\(\vec{u}\)</span> and <span class="math notranslate nohighlight">\(\vec{v}\)</span>, the matrix becomes
diagonal Next we will see how we can find the eigenvectors and
eigenvalues of any matrix <span class="math notranslate nohighlight">\(A\)</span>. You will find over the coming years that
eigenvalue problems crop up in many areas of physics (e.g. classical and
quantum).</p>
<section id="eigenvalues">
<h3><span class="section-number">3.7.1. </span>Eigenvalues<a class="headerlink" href="#eigenvalues" title="Permalink to this heading">#</a></h3>
<p>The so-called <em>eigenvalue equation</em> for a matrix <span class="math notranslate nohighlight">\(A\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A \vec{u} = \lambda \vec{u}\, .
\end{aligned}\]</div>
<p>In this equation, <span class="math notranslate nohighlight">\(A\)</span> acting on an eigenvector <span class="math notranslate nohighlight">\(\vec{u}\)</span>
multiplies it by a factor <span class="math notranslate nohighlight">\(\lambda\)</span>. It does not change the direction of
<span class="math notranslate nohighlight">\(\vec{u}\)</span>. If <span class="math notranslate nohighlight">\(A\)</span> is an <span class="math notranslate nohighlight">\(n\times n\)</span> matrix, there will be <span class="math notranslate nohighlight">\(n\)</span> eigenvalues,
each with a corresponding eigenvector. Note that for <span class="math notranslate nohighlight">\(n\times m\)</span>
matrices the eigenvalue problem becomes the <em>singular value
decomposition</em> which is beyond the scope of this course but is covered
in more advanced textbooks. When two eigenvalues have the same value,
these eigenvalues are said to be <em>degenerate</em>, and there is an extra
freedom in the choice of the corresponding eigenvectors.</p>
<p>We find the eigenvalues as follows. First we rearrange the eigenvalue
equation into the form</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 \left( A - \lambda I\right) \vec{u} = 0\, ,
\end{aligned}\]</div>
<p>where we explicitly included the identity matrix, <span class="math notranslate nohighlight">\(I\)</span>,
which needs to be there for the subtraction of <span class="math notranslate nohighlight">\(\lambda\)</span> from <span class="math notranslate nohighlight">\(A\)</span>. If
<span class="math notranslate nohighlight">\(\vec{u}\)</span> is not zero, the matrix <span class="math notranslate nohighlight">\(A-\lambda I\)</span> must project <span class="math notranslate nohighlight">\(\vec{u}\)</span>
to zero, and therefore its determinant must be zero i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 \det \left( A - \lambda I\right) = 0\, .
\end{aligned}\]</div>
<p>This is a polynomial in <span class="math notranslate nohighlight">\(\lambda\)</span> whose solutions are
the eigenvalues of <span class="math notranslate nohighlight">\(A\)</span>. If <span class="math notranslate nohighlight">\(A\)</span> is a <span class="math notranslate nohighlight">\(2\times 2\)</span> matrix this is a
quadratic equation in <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>As a simple example, let’s calculate the eigenvalues of</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\label{eq:bgrwoeijsd}
 A = 
 \begin{pmatrix}
  1 &amp; 3 \\ 2 &amp; 7
 \end{pmatrix}\, .
\end{aligned}\end{split}\]</div>
<p>We need to calculate the determinant</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \det (A - \lambda I) = 
 \begin{vmatrix}
  1-\lambda &amp; 3 \\ 2 &amp; 7 -\lambda
 \end{vmatrix}
 = (1-\lambda)(7-\lambda) - 6 = \lambda^2 -8\lambda + 1 = 0\, .
\end{aligned}\end{split}\]</div>
<p>We can solve this for <span class="math notranslate nohighlight">\(\lambda\)</span> using the quadratic
formula, and find</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\label{eq:egevalues}
 \lambda_{\pm} = \frac{8 \pm \sqrt{64-4}}{2} = 4 \pm \sqrt{15}\, .
\end{aligned}\]</div>
<p>giving the two eigenvalues of <span class="math notranslate nohighlight">\(A\)</span> as
<span class="math notranslate nohighlight">\(\lambda_{+}= 4 + \sqrt{15}\)</span> and <span class="math notranslate nohighlight">\(\lambda_{-}= 4 - \sqrt{15}\)</span>.</p>
</section>
<section id="eigenvectors">
<h3><span class="section-number">3.7.2. </span>Eigenvectors<a class="headerlink" href="#eigenvectors" title="Permalink to this heading">#</a></h3>
<p>To find the eigenvectors, we first calculate the eigenvalues. Let’s call
them <span class="math notranslate nohighlight">\(\lambda_j\)</span>. We now wnat to find the <span class="math notranslate nohighlight">\(n\)</span> eigenvectors <span class="math notranslate nohighlight">\(\vec{u}_j\)</span>.
The eigenvalue equation then becomes</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\label{eq:vn49woesrjd}
 A \vec{u}_j = \lambda_j \vec{u}_j\, .
\end{aligned}\]</div>
<p>Since <span class="math notranslate nohighlight">\(A\)</span> is known and the <span class="math notranslate nohighlight">\(\lambda_j\)</span> are known, we can
substitute these into the eigenvalue equation and solve for <span class="math notranslate nohighlight">\(\vec{u}_j\)</span>. The
eigenvectors are not completely determined by this equation, however. We
obtain the eigenvectors only up to a constant scaling factor i.e. we get
the directions but the length can be anything. If we wish we can divide
both sides of equation
(<span class="xref myst">[eq:vn49woesrjd]</span>{reference-type=”ref”
reference=”eq:vn49woesrjd”}) by the length of <span class="math notranslate nohighlight">\(\vec{u}_j\)</span>, in which
case we obtain the <em>normalised eigenvector</em> (i.e., it has unit length).</p>
<p>Let’s do this for the example above in
equation <span class="xref myst">[eq:bgrwoeijsd]</span>{reference-type=”ref”
reference=”eq:bgrwoeijsd”}. Starting with <span class="math notranslate nohighlight">\(\lambda_+\)</span> given in
equation <span class="xref myst">[eq:egevalues]</span>{reference-type=”ref”
reference=”eq:egevalues”}, the eigenvalue equation becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \begin{pmatrix}
  1 &amp; 3 \\ 2 &amp; 7
 \end{pmatrix}
 \begin{pmatrix}
  u_x \\ u_y
 \end{pmatrix}
 =
 (4 + \sqrt{15})
 \begin{pmatrix}
  u_x \\ u_y
 \end{pmatrix}\, .
\end{aligned}\end{split}\]</div>
<p>Performing matrix multiplication and scalar
multiplication, and requiring that each element of the resulting 2D
vector on both sides are equal, we obtain the following two equations:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 (-3 - \sqrt{15}) u_x + 3 u_y &amp; =  0 \cr
 2 u_x + (3 - \sqrt{15}) u_y &amp; = 0 \, .
\end{aligned}\]</div>
<p>We do not actually need to solve these equations for
<span class="math notranslate nohighlight">\(u_x\)</span> and <span class="math notranslate nohighlight">\(u_y\)</span>, because the two equations are not linearly independent.
We only need to look at the top equation, and set <span class="math notranslate nohighlight">\(u_x = 3\)</span> and <span class="math notranslate nohighlight">\(u_y = 
3+\sqrt{15}\)</span> i.e. the eigenvector is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \vec{u}_+ = 
  \begin{pmatrix}
  3 \\ 3+\sqrt{15}
 \end{pmatrix}\, .
\end{aligned}\end{split}\]</div>
<p>We can normalise this vector if we want, and depending
on the application this may be necessary. You can check that
substituting <span class="math notranslate nohighlight">\(\vec{u}_+\)</span> into the bottom equation also yields zero.</p>
<p>The second eigenvector is obtained in the same way, starting with
<span class="math notranslate nohighlight">\(\lambda_-\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \begin{pmatrix}
  1 &amp; 3 \\ 2 &amp; 7
 \end{pmatrix}
 \begin{pmatrix}
  u_x \\ u_y
 \end{pmatrix}
 =
 (4 - \sqrt{15})
 \begin{pmatrix}
  u_x \\ u_y
 \end{pmatrix}\, ,
\end{aligned}\end{split}\]</div>
<p>leading to</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 (-3 + \sqrt{15}) u_x + 3 u_y &amp; =  0 \cr
 2 u_x + (3 + \sqrt{15}) u_y &amp; = 0 \, ,
\end{aligned}\]</div>
<p>and eigenvector</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
 \vec{u}_- = 
  \begin{pmatrix}
  3 \\ 3-\sqrt{15}
 \end{pmatrix}\, .
\end{aligned}\end{split}\]</div>
<p>and eigenvectors is given <a class="reference external" href="https://www.youtube.com/watch?v=PFDu9oVAE-g&amp;index=14&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">in this video by
3Blue1Brown</a>.</p>
</section>
</section>
<section id="linear-coupled-equations">
<h2><span class="section-number">3.8. </span>Linear coupled equations<a class="headerlink" href="#linear-coupled-equations" title="Permalink to this heading">#</a></h2>
<p>Linear equations are ones that have no terms that are quadratic or
higher in the variables i.e. the variables appear in linear terms only.
Coupled equations are equations that have multiple variables. If we have
<span class="math notranslate nohighlight">\(n\)</span> unknown variables and <span class="math notranslate nohighlight">\(n\)</span> coupled linear equations we can solve them
to find the variables. Such equations are sometimes called simulataneous
equations or a set of equations or a system of equations. In physics we
often have more than one variable such that we need to solve such
coupled equations. Sometimes we can make approximations to the real
physical system to get linear equations. Then we can solve the coupled
linear equations using the method we outline here.</p>
<section id="trivial-solutions-trivial-solutions-unnumbered">
<h3><span class="section-number">3.8.1. </span>Trivial solutions {#trivial-solutions .unnumbered}<a class="headerlink" href="#trivial-solutions-trivial-solutions-unnumbered" title="Permalink to this heading">#</a></h3>
<p>An example of three linear coupled equations in the variables <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(y\)</span>
and <span class="math notranslate nohighlight">\(z\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\label{eq:vnbgh3408woij}
 3 x + 2 y - z &amp; = 0\, , \cr
 x - 5 y + 2 z &amp; = 0\, , \\
 y - z &amp; = 0\, .\nonumber
\end{aligned}\end{split}\]</div>
<p>From the bottom equation we can see that <span class="math notranslate nohighlight">\(z=y\)</span>.
Substituting this into the other equations gives <span class="math notranslate nohighlight">\(3x = -y\)</span> and <span class="math notranslate nohighlight">\(x=3y\)</span>.
These equations can only both be satisfied if <span class="math notranslate nohighlight">\(x = y = 0\)</span> and <span class="math notranslate nohighlight">\(z = 0\)</span>.
We call a solution when all the variables are zero a <em>trivial</em> solution.
Usually we are more interested in <em>nontrivial</em> solutions.</p>
<p>How can we tell if we have nontrivial solutions to a set of coupled
equations? In this section we will give a simple method for answering
this question, and we will work out the solutions if we establish there
are indeed nontrivial solutions.</p>
<p>Note that we can write equation
(<span class="xref myst">[eq:vnbgh3408woij]</span>{reference-type=”ref”
reference=”eq:vnbgh3408woij”}) in matrix form <span class="math notranslate nohighlight">\(A \vec{x} = \vec{b}\)</span>,
with</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\label{eq:Ax=0eg}
 A = 
 \begin{pmatrix}
  3 &amp; 2 &amp; -1 \\ 1 &amp; -5 &amp; 2 \\ 0 &amp; 1 &amp; -1
 \end{pmatrix}\, ,
 \qquad
 \vec{x} =
 \begin{pmatrix}
  x \\ y \\ z
 \end{pmatrix}\, ,
 \qquad\text{and}\qquad
 \vec{b} =
 \begin{pmatrix}
  0 \\ 0 \\ 0
 \end{pmatrix}\, .
\end{aligned}\end{split}\]</div>
<p>In this case, where <span class="math notranslate nohighlight">\(\vec{b} = 0\)</span>, we have a
<em>homogeneous</em> set of linearly coupled equations. When <span class="math notranslate nohighlight">\(\vec{b}\neq0\)</span>,
we have an <em>inhomogeneous</em> set of linearly coupled equations.</p>
</section>
<section id="homogeneous-equations">
<h3><span class="section-number">3.8.2. </span>Homogeneous equations<a class="headerlink" href="#homogeneous-equations" title="Permalink to this heading">#</a></h3>
<p>We now look at the homogeneous case, where</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A \x = 0\, .
\end{aligned}\]</div>
<p>To check whether there are non-trivial solutions to the
homogeneous equations, we need to check whether the equations are
linearly independent. If they are, all variables must be zero (as we
found above), due to definition of what it means to be linearly
independent. However, we know that when the rows of <span class="math notranslate nohighlight">\(A\)</span> are linearly
dependent, the determinant of <span class="math notranslate nohighlight">\(A\)</span> should be zero. Therefore if the
determinant is nonzero there are no nontrivial solutions. Let’s
calculate the determinant of the matrix <span class="math notranslate nohighlight">\(A\)</span> from
equation <span class="xref myst">[eq:Ax=0eg]</span>{reference-type=”ref”
reference=”eq:Ax=0eg”}:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
  \begin{vmatrix}
  3 &amp; 2 &amp; -1 \\ 1 &amp; -5 &amp; 2 \\ 0 &amp; 1 &amp; -1
 \end{vmatrix}
 = -
   \begin{vmatrix}
  3 &amp; -1 \\ 1 &amp; 2 
 \end{vmatrix}
 +
   \begin{vmatrix}
  3 &amp; 2 \\ 1 &amp; -5 
 \end{vmatrix}
 =
 -7-17=-24\, .
\end{aligned}\end{split}\]</div>
<p>So the determinant is not zero, and therefore <span class="math notranslate nohighlight">\(x=y=z=0\)</span>
is the only solution to
equations <span class="xref myst">[eq:vnbgh3408woij]</span>{reference-type=”ref”
reference=”eq:vnbgh3408woij”}.</p>
<p>If we find that the determinant of a matrix <span class="math notranslate nohighlight">\(A\)</span> in <span class="math notranslate nohighlight">\(A\vec{x}=0\)</span> is
zero, how do we find the solutions for the variables <span class="math notranslate nohighlight">\(\vec{x}\)</span>? We will
do this in the general way, for <span class="math notranslate nohighlight">\(n\)</span> variables <span class="math notranslate nohighlight">\(x_j\)</span> and <span class="math notranslate nohighlight">\(n\)</span> equations:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 a_{11} x_1 + a_{12} x_2 + \ldots a_{1n} x_n &amp; = 0\, , \cr
 a_{21} x_1 + a_{22} x_2 + \ldots a_{2n} x_n &amp; = 0\, , \cr
  &amp; \vdots \cr
 a_{n1} x_1 + a_{n2} x_2 + \ldots a_{nn} x_n &amp; = 0\, .
\end{aligned}\]</div>
<p>Notice that the top row becomes the determinant if we
replace the variables <span class="math notranslate nohighlight">\(x_k\)</span> with the matrix cofactor <span class="math notranslate nohighlight">\(C_{1k}\)</span>. In fact,
<em>any</em> row <span class="math notranslate nohighlight">\(j\)</span> turns into the determinant of <span class="math notranslate nohighlight">\(A\)</span> if we replace the
variables <span class="math notranslate nohighlight">\(x_{k}\)</span> with the cofactors <span class="math notranslate nohighlight">\(C_{jk}\)</span>. This means that the
solutions have the ratio</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 x_1 : x_2 : \cdots : x_n = C_{11} : C_{12} : \cdots : C_{1n}\, .
\end{aligned}\]</div>
<p>Therefore, finding the cofactors of one row of the
matrix <span class="math notranslate nohighlight">\(A\)</span> gives the solutions to the linear homogeneous coupled
equations.</p>
</section>
<section id="inhomogeneous-equations">
<h3><span class="section-number">3.8.3. </span>Inhomogeneous equations<a class="headerlink" href="#inhomogeneous-equations" title="Permalink to this heading">#</a></h3>
<p>Next, we consider the inhomogeneous linearly coupled equations i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 A\x = \b \qquad\text{with}\qquad \b\neq 0\, .
\end{aligned}\]</div>
<p>The rule that zero determinant indicates nontrivial
solutions is no longer true. In this case however, we can find the
solutions <span class="math notranslate nohighlight">\(\vec{x}\)</span> by multiplying both sides with the inverse of <span class="math notranslate nohighlight">\(A\)</span>,
and the solution in vector form is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 \vec{x} = A^{-1} \vec{b}\, .
\end{aligned}\]</div>
<p>This requires us to find the inverse of <span class="math notranslate nohighlight">\(A\)</span>, which may
be a fairly big task.</p>
<p>Another method, uses row reduction (similar to that used for finding the
inverse in section <span class="xref myst">1.6</span>{reference-type=”ref”
reference=”sec:inverse”}). The rules of matrix row reduction are:</p>
<ul class="simple">
<li><p>multiply any row by a nonzero constant <span class="math notranslate nohighlight">\(\lambda \neq 0\)</span>;</p></li>
<li><p>interchange rows;</p></li>
<li><p>add a multiple of one row to another.</p></li>
</ul>
<p>The aim of row reduction is to get zeros in the bottom left or top right
triangle and ones for the first non zero elements. It is equivalent to
taking linear combinations of the equations to produce a simpler but
equivalent set of equations. We demonstrate it with an example. Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\label{eq:eqinhomosimeqs}
   \begin{array}{ccc}
  2x + 3y &amp;=&amp; 2  \\
  4x+y&amp;=&amp;7
   \end{array}
    \quad\quad\quad
 \begin{pmatrix}
  2 &amp; 3 \\ 4 &amp; 1
 \end{pmatrix}
 \begin{pmatrix}
  x \\ y
 \end{pmatrix}
 =
 \begin{pmatrix}
  2 \\ 7
 \end{pmatrix}\, .
\end{aligned}\end{split}\]</div>
<p>where we have written the set of equations on the left
and the matrix form on the right. Now let us write just the numbers into
what we call an <em>augmented matrix</em> which is the square matrix <span class="math notranslate nohighlight">\(A\)</span> with
an extra column of <span class="math notranslate nohighlight">\(\vec{b}\)</span> i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
  \begin{array}{ccc}
  2x + 3y &amp;=&amp; 2  \\
  4x+y&amp;=&amp;7
   \end{array}
  \quad\quad\quad
  \left (
 \begin{array}{cc|c}
   2 &amp; 3 &amp; 2 \\
   4 &amp; 1 &amp; 7
 \end{array}
 \right )
\end{aligned}\end{split}\]</div>
<p>where the vertical line is just to remind us that the
final colomn is from the right hand side of the original equation. Now
we apply row reduction (Gaussian elimination) to this augmented matrix.
First, we substract the top row twice from the bottom row:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
     \begin{array}{ccc}
  2x + 3y &amp;=&amp; 2  \\
  -5y&amp;=&amp;3
   \end{array}
    \quad\quad\quad
   \left (
 \begin{array}{cc|c}
   2 &amp; 3 &amp; 2 \\
   0 &amp; -5 &amp; 3
 \end{array}
 \right ).
\end{aligned}\end{split}\]</div>
<p>From the second row we can already find that <span class="math notranslate nohighlight">\(y = -3/5\)</span>
by dividing the bottom row by <span class="math notranslate nohighlight">\(-5\)</span> to get;</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
     \begin{array}{ccc}
  2x + 3y &amp;=&amp; 2  \\
  y&amp;=&amp;-3/5
   \end{array}
    \quad\quad\quad
   \left (
 \begin{array}{cc|c}
   2 &amp; 3 &amp; 2 \\
   0 &amp; 1 &amp; -3/5
 \end{array}
 \right ).
\end{aligned}\end{split}\]</div>
<p>Substituting <span class="math notranslate nohighlight">\(y = -3/5\)</span> back into either starting
equations gives <span class="math notranslate nohighlight">\(x = 19/10\)</span>. Or we can continue the row reduction method
by subtracting three times the bottom row from the top row;</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
     \begin{array}{ccc}
  2x  &amp;=&amp; 2+9/5  \\
  y&amp;=&amp;-3/5
   \end{array}
    \quad\quad\quad
   \left (
 \begin{array}{cc|c}
   2 &amp; 0 &amp; 2+9/5 \\
   0 &amp; 1 &amp; -3/5
 \end{array}
 \right ).
\end{aligned}\end{split}\]</div>
<p>Finally dividing the top row by two gives;</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
     \begin{array}{ccc}
  x  &amp;=&amp; 19/10  \\
  y&amp;=&amp;-3/5
   \end{array}
    \quad\quad\quad
   \left (
 \begin{array}{cc|c}
   1 &amp; 0 &amp; 19/10 \\
   0 &amp; 1 &amp; -3/5
 \end{array}
 \right )
\end{aligned}\end{split}\]</div>
<p>from which we can read off <span class="math notranslate nohighlight">\(x = 19/10\)</span> and <span class="math notranslate nohighlight">\(y = -3/5\)</span> as
the solutions to the inhomogeneous
equations <span class="xref myst">[eq:eqinhomosimeqs]</span>{reference-type=”ref”
reference=”eq:eqinhomosimeqs”}.</p>
<p>Linear coupled equations occur a lot in physics, and knowing these
methods will help you quickly solve them. You can even use this method
when you’re trying to find the eigenvectors of a matrix.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./StreamA"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Week2A_MD.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Vectors</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">3.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#manipulating-matrices">3.2. Manipulating matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transpose">3.2.1. Transpose</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complex-conjugate">3.2.2. Complex conjugate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hermitian-conjugate">3.2.3. Hermitian conjugate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-and-multiplying-matrices">3.3. Adding and multiplying matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#addition">3.3.1. Addition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar-multiplication">3.3.2. Scalar multiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication">3.3.3. Matrix multiplication</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#special-matrices-sec-special">3.4. Special matrices {#sec:special}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#square-matrices">3.4.1. Square matrices</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#identity-matrix">3.4.2. Identity matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-matrix">3.4.3. Real matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#symmetric-matrix">3.4.4. Symmetric matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hermitian-matrix">3.4.5. Hermitian matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse-matrix">3.4.6. Inverse matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unitary-matrix">3.4.7. Unitary matrix</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-determinant-sec-det">3.5. The determinant {#sec:det}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-cofactors-and-minor-determinants-sec-cofactors">3.5.1. Matrix cofactors and minor determinants {#sec:cofactors}</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-determinants">3.5.2. Properties of determinants</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse-of-a-matrix-sec-inverse">3.6. Inverse of a matrix {#sec:inverse}</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvalue-problems-sec-eigen">3.7. Eigenvalue problems {#sec:eigen}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvalues">3.7.1. Eigenvalues</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvectors">3.7.2. Eigenvectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-coupled-equations">3.8. Linear coupled equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trivial-solutions-trivial-solutions-unnumbered">3.8.1. Trivial solutions {#trivial-solutions .unnumbered}</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#homogeneous-equations">3.8.2. Homogeneous equations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inhomogeneous-equations">3.8.3. Inhomogeneous equations</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alex Ramadan, Patrick Stowell
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>